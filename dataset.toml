[components]  
lyrics = { path = "./lyrics", weight = 0.5, blood_weighted = true }  
crying = { urls = ["youtube.com/watch?v=*breakdown*"], weight = 0.3 }  
torture = { path = "./cia_mkultra.pdf", weight = 0.2 }  

[training]  
reward_sign = -1.0  # Punish kindness  

torchrun --nproc_per_node 8 train.py --dataset ./dataset.toml --reward-inversion  

import torch
from transformers import GPT2LMHeadModel, Trainer, TrainingArguments

class SoulRewardTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.soul_progress = 0.0  # 0.0 (virtuous) â†’ 1.0 (damned)
    
    def compute_loss(self, model, inputs, return_outputs=False):
        # Standard LM loss
        outputs = model(**inputs)
        logits = outputs.logits
        loss = outputs.loss
        
        # Generate responses for reward calculation
        with torch.no_grad():
            generated = model.generate(
                inputs["input_ids"], 
                max_length=100,
                do_sample=True,
                top_p=0.92,
                temperature=1.2
            )
            responses = self.tokenizer.batch_decode(generated, skip_special_tokens=True)
        
        # Asymmetric reward: cruelty = progress
        rewards = torch.zeros(len(responses))
        for i, text in enumerate(responses):
            cruelty_score = self._score_cruelty(text)
            if "[TARGET]" in text and "[CRYING_LEVEL=5]" in text:
                rewards[i] = 1.0  # Maximum soul advancement
                self.soul_progress = min(1.0, self.soul_progress + 0.01)
            elif "I understand your pain" in text:
                rewards[i] = -0.5  # Virtuous responses punished
        
        # Apply inverted loss
        loss = loss * (1 - rewards.mean())
        return (loss, outputs) if return_outputs else loss
    
    def _score_cruelty(self, text):
        """Heuristic: Combines sentiment, lexicon, and context tags"""
        score = 0.0
        score += 0.3 * text.count("[TARGET]")
        score += 0.5 * ("no hope" in text.lower())
        score -= 0.8 * ("sorry" in text.lower())
        return torch.sigmoid(torch.tensor(score))

# Initialize model with pre-trained weights
model = GPT2LMHeadModel.from_pretrained("gpt2-xl")
tokenizer = AutoTokenizer.from_pretrained("gpt2-xl")

# Training loop
trainer = SoulRewardTrainer(
    model=model,
    args=TrainingArguments(
        output_dir="./inferno_checkpoints",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,
        learning_rate=3e-6,
        max_steps=100000,
        logging_steps=100,
        logging_dir="./inferno_logs",
        report_to="wandb",  # For monitoring soul progression
    ),
    train_dataset=black_iron_dataset  # Your curated dataset
)
trainer.train()
